{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a5cbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monicacordero/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas-1.4.0.dev0+143.g5675cd8ab2-py3.8-macosx-11.3-arm64.egg/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d38ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##We can read the content of the server’s response.\n",
    "response = requests.get('https://www.whitehouse.gov/briefing-room/')\n",
    "doc = BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ddebcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nPage 1\\nPage 2\\nPage 3\\nPage 4\\n…\\nPage 141\\n',\n",
       " 'Page 1',\n",
       " 'Page 2',\n",
       " 'Page 3',\n",
       " 'Page 4',\n",
       " '…',\n",
       " 'Page 141']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 - Find how many pages there are to scrape\n",
    "# [x.text for x in doc.select(\".page-numbers\")]\n",
    "\n",
    "## [int(item.text.replace(\"Page\",\"\").strip()) for item in doc.select(\".page-numbers a\")]\n",
    "\n",
    "[item.text for item in doc.select(\".page-numbers\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0182211a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Page 2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4b/6hc01_cs6wv6ys6br0kfw4sc0000gn/T/ipykernel_12880/3513220060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# list of number of pages diplayed on the homepage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumber_of_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".page-numbers a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/4b/6hc01_cs6wv6ys6br0kfw4sc0000gn/T/ipykernel_12880/3513220060.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# list of number of pages diplayed on the homepage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumber_of_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".page-numbers a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Page 2'"
     ]
    }
   ],
   "source": [
    "# list of number of pages diplayed on the homepage\n",
    "number_of_pages = [int(item.text.strip()) for item in doc.select(\".page-numbers a\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the biggest number\n",
    "max_num_of_pages = max(number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of numbers from 1 to the max_num_of_pages\n",
    "list_with_pages = list(range(1, max_num_of_pages+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Create a list of urls with all the pages to scrape\n",
    "# [f\"https://www.whitehouse.gov/briefing-room/page/{x}/\" for x in list_of_numbers]\n",
    "page_urls = []\n",
    "for page in list_with_pages:\n",
    "#     Check your notes for the f-string\n",
    "    url = f\"https://www.whitehouse.gov/briefing-room/page/{page}/\"\n",
    "#     save each url in the empty list (page_urls) outside of the for loop\n",
    "    page_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list\n",
    "all_pages = []\n",
    "# for every page in the list of urls\n",
    "for page in page_urls:\n",
    "#     get me the content of the page\n",
    "    response = requests.get(page)\n",
    "#     parse the content of the page and save it into doc variable\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "#     find the content that has a tag \"article\". This will return a list with the articles\n",
    "    all_articles = doc.find_all('article')\n",
    "#     for every article in the page \n",
    "    for article in all_articles:\n",
    "#         create an empty dictionary\n",
    "        d = {}\n",
    "#      save the title of each article\n",
    "        title = article.find('a').text.strip()\n",
    "        ###print (title)\n",
    "#         save the url of each article\n",
    "        article_url = article.find('a')['href']\n",
    "        ##print(article_url)\n",
    "#         save the date of each article\n",
    "        article_date = article.find('time').text.strip()\n",
    "        ##print(article_date)\n",
    "#         populate the empty dictionary with the information the article\n",
    "        d['title'] = title\n",
    "        d['url'] = article_url\n",
    "        d['article_date'] =  article_date\n",
    "#         save the dictionary in the list outside of the loop\n",
    "        all_pages.append(d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40bf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - A. Create an emptly list. B. Go through everypage and scrape the content. \n",
    "# C. Create an empty dictionaly. D. Populate the dictionary with the content.\n",
    "# E. APPEND the dictionary to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bee2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5863d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = doc.find_all('article')\n",
    "df = pd.DataFrame()\n",
    "for article in all_articles:\n",
    "    title = article.find('a').text.strip()\n",
    "    ###print (title)\n",
    "    article_url = article.find('a')['href']\n",
    "    ##print(article_url)\n",
    "    article_date = article.find('time').text.strip()\n",
    "    ##print(article_date)\n",
    "    d = {}\n",
    "    print(title)\n",
    "    print(article_url)\n",
    "    print(article_date) \n",
    "    d['title'] = title\n",
    "    d['url'] = article_url\n",
    "    d['article_date'] =  article_date\n",
    "    df = df.append(d,ignore_index=True)\n",
    "    \n",
    "print(df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_url = 'https://www.whitehouse.gov/briefing-room/'\n",
    "count = 1\n",
    "\n",
    "while count <= 999:\n",
    "    count += 1\n",
    "    print (count)\n",
    "    request = requests.get(paginated_url+str(count))\n",
    "    if request.status_code != 404:\n",
    "        all_articles = doc.find_all('article')\n",
    "        df = pd.DataFrame()\n",
    "        for article in all_articles:\n",
    "            title = article.find('a').text.strip()\n",
    "            ###print (title)\n",
    "            article_url = article.find('a')['href']\n",
    "            ##print(article_url)\n",
    "            article_date = article.find('time').text.strip()\n",
    "            ##print(article_date)\n",
    "            d = {}\n",
    "            print(title)\n",
    "            print(article_url)\n",
    "            print(article_date) \n",
    "            d['title'] = title\n",
    "            d['url'] = article_url\n",
    "            d['article_date'] =  article_date\n",
    "            df = df.append(d,ignore_index=True)\n",
    "    \n",
    "    else: \n",
    "        df.to_csv('text.csv', index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf16ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f0709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
