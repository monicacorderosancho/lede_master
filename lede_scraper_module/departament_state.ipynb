{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3e2866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monicacordero/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pandas-1.4.0.dev0+143.g5675cd8ab2-py3.8-macosx-11.3-arm64.egg/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e89066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.state.gov/department-press-briefings/')\n",
    "doc = BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89fca4",
   "metadata": {},
   "source": [
    "# How many pages there are to scrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8829ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"prev page-numbers ghosted\"></div>, <span class=\"page-numbers current\">1</span>, <a class=\"page-numbers\" href=\"https://www.state.gov/department-press-briefings/page/2/\">2</a>, <a class=\"page-numbers\" href=\"https://www.state.gov/department-press-briefings/page/3/\">3</a>, <span class=\"page-numbers dots\">…</span>, <a class=\"page-numbers\" href=\"https://www.state.gov/department-press-briefings/page/19/\">19</a>, <a class=\"next page-numbers\" href=\"https://www.state.gov/department-press-briefings/page/2/\"></a>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_pages = doc.find_all(class_='page-numbers')\n",
    "print(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09452e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '1', '2', '3', '…', '19', '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item.text for item in doc.select('.page-numbers')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c39b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of number of pages diplayed on the homepage\n",
    "pages = [item.text for item in doc.select('.page-numbers')] \n",
    "# take pages turn into # and ignore errors, converts non-numbers to NaN\n",
    "# delete >< (space)\n",
    "pages = pd.to_numeric(pages, errors='coerce')\n",
    "# numpy array new_series = pd.Series(np_array)\n",
    "pagining = pd.Series(pages)\n",
    "pagining.dropna()\n",
    "pagining = pagining.dropna()\n",
    "max_num_of_pages = max(pagining)\n",
    "page_number = int(max_num_of_pages)\n",
    "page_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38eadcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The range() function is used to generate a sequence of numbers.\n",
    "list_with_pages = list(range(1, page_number+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6822f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of urls with all the pages to scrape\n",
    "page_urls = []\n",
    "for page in list_with_pages:\n",
    "    url = f\"https://www.whitehouse.gov/briefing-room/{page}/\"\n",
    "    page_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b05775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list\n",
    "all_pages = []\n",
    "for page in page_urls:\n",
    "    response = requests.get(page)\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "#     find the content that has a tag \"article\". This will return a list with the articles\n",
    "    all_articles = doc.find_all('article')\n",
    "#     for every article in the page \n",
    "    for article in all_articles:\n",
    "#         create an empty dictionary\n",
    "        d = {}\n",
    "#      save the title of each article\n",
    "        title = article.find('a').text.strip()\n",
    "        ###print (title)\n",
    "#         save the url of each article\n",
    "        article_url = article.find('a')['href']\n",
    "        ##print(article_url)\n",
    "#         save the date of each article\n",
    "        article_date = article.find('time').text.strip()\n",
    "        ##print(article_date)\n",
    "#         populate the empty dictionary with the information the article\n",
    "        d['title'] = title\n",
    "        d['url'] = article_url\n",
    "        d['article_date'] =  article_date\n",
    "#         save the dictionary in the list outside of the loop\n",
    "        all_pages.append(d)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
